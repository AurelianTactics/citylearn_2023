{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsee obs\\nsee action space\\nsee render\\nthink how to SA it\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a gym env\n",
    "#imple iter\n",
    "#kload schemas\n",
    "'''\n",
    "see obs\n",
    "see action space\n",
    "see render\n",
    "think how to SA it\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/intelligent-environments-lab/CityLearn/blob/citylearn_2022/challenge/agent.py\n",
    "#maybe useful for central agent stuff?\n",
    "\n",
    "\n",
    "# https://gitlab.aicrowd.com/aicrowd/challenges/citylearn-challenge-2022/citylearn-2022-starter-kit/-/blob/master/local_evaluation.py\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# \"\"\"\n",
    "# Please do not make changes to this file. \n",
    "# This is only a reference script provided to allow you \n",
    "# to do local evaluation. The evaluator **DOES NOT** \n",
    "# use this script for orchestrating the evaluations. \n",
    "# \"\"\"\n",
    "\n",
    "# from agents.orderenforcingwrapper import OrderEnforcingAgent\n",
    "# from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "# class Constants:\n",
    "#     episodes = 3\n",
    "#     schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "# def action_space_to_dict(aspace):\n",
    "#     \"\"\" Only for box space \"\"\"\n",
    "#     return { \"high\": aspace.high,\n",
    "#              \"low\": aspace.low,\n",
    "#              \"shape\": aspace.shape,\n",
    "#              \"dtype\": str(aspace.dtype)\n",
    "#     }\n",
    "\n",
    "# def env_reset(env):\n",
    "#     observations = env.reset()\n",
    "#     action_space = env.action_space\n",
    "#     action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "#     obs_dict = {\"action_space\": action_space_dicts,\n",
    "#                 \"observation\": observations }\n",
    "#     return obs_dict\n",
    "\n",
    "# def evaluate():\n",
    "#     print(\"Starting local evaluation\")\n",
    "    \n",
    "#     env = CityLearnEnv(schema=Constants.schema_path)\n",
    "#     agent = OrderEnforcingAgent()\n",
    "\n",
    "#     obs_dict = env_reset(env)\n",
    "\n",
    "#     agent_time_elapsed = 0\n",
    "\n",
    "#     step_start = time.perf_counter()\n",
    "#     actions = agent.register_reset(obs_dict)\n",
    "#     agent_time_elapsed += time.perf_counter()- step_start\n",
    "\n",
    "#     episodes_completed = 0\n",
    "#     num_steps = 0\n",
    "#     interrupted = False\n",
    "#     episode_metrics = []\n",
    "#     try:\n",
    "#         while True:\n",
    "#             observations, _, done, _ = env.step(actions)\n",
    "#             if done:\n",
    "#                 episodes_completed += 1\n",
    "#                 metrics_t = env.evaluate()\n",
    "#                 metrics = {\"price_cost\": metrics_t[0], \"emmision_cost\": metrics_t[1]}\n",
    "#                 if np.any(np.isnan(metrics_t)):\n",
    "#                     raise ValueError(\"Episode metrics are nan, please contant organizers\")\n",
    "#                 episode_metrics.append(metrics)\n",
    "#                 print(f\"Episode complete: {episodes_completed} | Latest episode metrics: {metrics}\", )\n",
    "\n",
    "#                 obs_dict = env_reset(env)\n",
    "\n",
    "#                 step_start = time.perf_counter()\n",
    "#                 actions = agent.register_reset(obs_dict)\n",
    "#                 agent_time_elapsed += time.perf_counter()- step_start\n",
    "#             else:\n",
    "#                 step_start = time.perf_counter()\n",
    "#                 actions = agent.compute_action(observations)\n",
    "#                 agent_time_elapsed += time.perf_counter()- step_start\n",
    "            \n",
    "#             num_steps += 1\n",
    "#             if num_steps % 1000 == 0:\n",
    "#                 print(f\"Num Steps: {num_steps}, Num episodes: {episodes_completed}\")\n",
    "\n",
    "#             if episodes_completed >= Constants.episodes:\n",
    "#                 break\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"========================= Stopping Evaluation =========================\")\n",
    "#         interrupted = True\n",
    "    \n",
    "#     if not interrupted:\n",
    "#         print(\"=========================Completed=========================\")\n",
    "\n",
    "#     if len(episode_metrics) > 0:\n",
    "#         print(\"Average Price Cost:\", np.mean([e['price_cost'] for e in episode_metrics]))\n",
    "#         print(\"Average Emmision Cost:\", np.mean([e['emmision_cost'] for e in episode_metrics]))\n",
    "#     print(f\"Total time taken by agent: {agent_time_elapsed}s\")\n",
    "    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmisc notes on the below\\nso not a reward in the local evaluator. uses a metrics thing instead\\n\\tdefault reward is 5 negative numbers (maybe for each building?)\\n\\tdefault obs is 5, 28\\nI don't get what the metrics thing is\\nunclear what the observations are, they are not scaled either\\n\\nin the obs_dict is the action space\\n    all teh shapes seem to be 1 (but I thought the buildings could have multiple actions?) maybe it's for different buildings?\\n    \\nquestions:\\ndiffere action spaces for different buildings?\\nsingle agent example?\\nhow is it for 7 buildings? how is it for the validation?\\nwhere to line up the obs with teh obs dict?\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "misc notes on the below\n",
    "so not a reward in the local evaluator. uses a metrics thing instead\n",
    "\tdefault reward is 5 negative numbers (maybe for each building?)\n",
    "\tdefault obs is 5, 28\n",
    "I don't get what the metrics thing is\n",
    "unclear what the observations are, they are not scaled either\n",
    "\n",
    "in the obs_dict is the action space\n",
    "    all teh shapes seem to be 1 (but I thought the buildings could have multiple actions?) maybe it's for different buildings?\n",
    "    \n",
    "questions:\n",
    "differe action spaces for different buildings?\n",
    "single agent example?\n",
    "how is it for 7 buildings? how is it for the validation?\n",
    "where to line up the obs with teh obs dict?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 13 positional arguments but 16 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      6\u001b[0m     schema_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jim/projects/citylearn_2023/citylearn-2023-starter-kit/data/schemas/warm_up/schema.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mCityLearnEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/citylearn_2022/lib/python3.9/site-packages/citylearn/citylearn.py:37\u001b[0m, in \u001b[0;36mCityLearnEnv.__init__\u001b[0;34m(self, schema, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m schema\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseconds_per_time_step,\\\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentral_agent, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/citylearn_2022/lib/python3.9/site-packages/citylearn/citylearn.py:723\u001b[0m, in \u001b[0;36mCityLearnEnv.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m building_schema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# data\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     energy_simulation \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot_directory\u001b[39m\u001b[38;5;124m'\u001b[39m],building_schema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_simulation\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39miloc[simulation_start_time_step:simulation_end_time_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 723\u001b[0m     energy_simulation \u001b[38;5;241m=\u001b[39m \u001b[43mEnergySimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menergy_simulation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m     weather \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot_directory\u001b[39m\u001b[38;5;124m'\u001b[39m],building_schema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweather\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39miloc[simulation_start_time_step:simulation_end_time_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    725\u001b[0m     weather \u001b[38;5;241m=\u001b[39m Weather(\u001b[38;5;241m*\u001b[39mweather\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 13 positional arguments but 16 were given"
     ]
    }
   ],
   "source": [
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "#/home/jim/projects/citylearn_2023/citylearn-2023-starter-kit/data/schemas/warm_up/schema.json\n",
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = '/home/jim/projects/citylearn_2023/citylearn-2023-starter-kit/data/schemas/warm_up/schema.json'\n",
    "    \n",
    "env = CityLearnEnv(schema=Constants.schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morderenforcingwrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderEnforcingAgent\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcitylearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcitylearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CityLearnEnv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConstants\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agents'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from agents.orderenforcingwrapper import OrderEnforcingAgent\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n",
    "\n",
    "\n",
    "env = CityLearnEnv(schema=Constants.schema_path)\n",
    "agent = OrderEnforcingAgent()\n",
    "\n",
    "obs_dict = env_reset(env)\n",
    "\n",
    "agent_time_elapsed = 0\n",
    "\n",
    "step_start = time.perf_counter()\n",
    "actions = agent.register_reset(obs_dict)\n",
    "agent_time_elapsed += time.perf_counter()- step_start\n",
    "\n",
    "episodes_completed = 0\n",
    "num_steps = 0\n",
    "interrupted = False\n",
    "episode_metrics = []\n",
    "\n",
    "#while True:\n",
    "for i in range(10):\n",
    "    observations, reward, done, info = env.step(actions)\n",
    "    print(i, np.shape(observations), reward, done, info)\n",
    "    print(i, np.shape(observations), np.shape(reward), np.shape(done), info)\n",
    "    print(\"obs_dict is\")\n",
    "    print(obs_dict)\n",
    "    print(\"obs is\")\n",
    "    print(observations)\n",
    "    done = True # testing stuff in the break\n",
    "    if done:\n",
    "        episodes_completed += 1\n",
    "        metrics_t = env.evaluate()\n",
    "        metrics = {\"price_cost\": metrics_t[0], \"emmision_cost\": metrics_t[1]}\n",
    "        if np.any(np.isnan(metrics_t)):\n",
    "            raise ValueError(\"Episode metrics are nan, please contant organizers\")\n",
    "        episode_metrics.append(metrics)\n",
    "        print(f\"Episode complete: {episodes_completed} | Latest episode metrics: {metrics}\", )\n",
    "\n",
    "        obs_dict = env_reset(env)\n",
    "\n",
    "        step_start = time.perf_counter()\n",
    "        actions = agent.register_reset(obs_dict)\n",
    "        agent_time_elapsed += time.perf_counter()- step_start\n",
    "        break\n",
    "    else:\n",
    "        step_start = time.perf_counter()\n",
    "        actions = agent.compute_action(observations)\n",
    "        agent_time_elapsed += time.perf_counter()- step_start\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price_cost': 1.0, 'emmision_cost': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_space': [{'high': array([2.], dtype=float32),\n",
       "   'low': array([-2.], dtype=float32),\n",
       "   'shape': (1,),\n",
       "   'dtype': 'float32'},\n",
       "  {'high': array([2.], dtype=float32),\n",
       "   'low': array([-2.], dtype=float32),\n",
       "   'shape': (1,),\n",
       "   'dtype': 'float32'},\n",
       "  {'high': array([2.], dtype=float32),\n",
       "   'low': array([-2.], dtype=float32),\n",
       "   'shape': (1,),\n",
       "   'dtype': 'float32'},\n",
       "  {'high': array([2.], dtype=float32),\n",
       "   'low': array([-2.], dtype=float32),\n",
       "   'shape': (1,),\n",
       "   'dtype': 'float32'},\n",
       "  {'high': array([2.], dtype=float32),\n",
       "   'low': array([-2.], dtype=float32),\n",
       "   'shape': (1,),\n",
       "   'dtype': 'float32'}],\n",
       " 'observation': [[8,\n",
       "   1,\n",
       "   0,\n",
       "   20.0,\n",
       "   18.3,\n",
       "   22.8,\n",
       "   20.0,\n",
       "   84.0,\n",
       "   81.0,\n",
       "   68.0,\n",
       "   81.0,\n",
       "   0.0,\n",
       "   25.0,\n",
       "   964.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   100.0,\n",
       "   815.0,\n",
       "   0.0,\n",
       "   0.1707244126403808,\n",
       "   2.2758,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2.2758,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22],\n",
       "  [8,\n",
       "   1,\n",
       "   0,\n",
       "   20.0,\n",
       "   18.3,\n",
       "   22.8,\n",
       "   20.0,\n",
       "   84.0,\n",
       "   81.0,\n",
       "   68.0,\n",
       "   81.0,\n",
       "   0.0,\n",
       "   25.0,\n",
       "   964.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   100.0,\n",
       "   815.0,\n",
       "   0.0,\n",
       "   0.1707244126403808,\n",
       "   2.18875,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2.18875,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22],\n",
       "  [8,\n",
       "   1,\n",
       "   0,\n",
       "   20.0,\n",
       "   18.3,\n",
       "   22.8,\n",
       "   20.0,\n",
       "   84.0,\n",
       "   81.0,\n",
       "   68.0,\n",
       "   81.0,\n",
       "   0.0,\n",
       "   25.0,\n",
       "   964.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   100.0,\n",
       "   815.0,\n",
       "   0.0,\n",
       "   0.1707244126403808,\n",
       "   1.0096232096354177e-07,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   1.0096232096354177e-07,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22],\n",
       "  [8,\n",
       "   1,\n",
       "   0,\n",
       "   20.0,\n",
       "   18.3,\n",
       "   22.8,\n",
       "   20.0,\n",
       "   84.0,\n",
       "   81.0,\n",
       "   68.0,\n",
       "   81.0,\n",
       "   0.0,\n",
       "   25.0,\n",
       "   964.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   100.0,\n",
       "   815.0,\n",
       "   0.0,\n",
       "   0.1707244126403808,\n",
       "   2.81915,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   2.81915,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22],\n",
       "  [8,\n",
       "   1,\n",
       "   0,\n",
       "   20.0,\n",
       "   18.3,\n",
       "   22.8,\n",
       "   20.0,\n",
       "   84.0,\n",
       "   81.0,\n",
       "   68.0,\n",
       "   81.0,\n",
       "   0.0,\n",
       "   25.0,\n",
       "   964.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   100.0,\n",
       "   815.0,\n",
       "   0.0,\n",
       "   0.1707244126403808,\n",
       "   0.7714333333333336,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.7714333333333336,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22,\n",
       "   0.22]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8,\n",
       "  1,\n",
       "  1,\n",
       "  20.1,\n",
       "  19.4,\n",
       "  22.8,\n",
       "  19.4,\n",
       "  79.0,\n",
       "  79.0,\n",
       "  71.0,\n",
       "  87.0,\n",
       "  0.0,\n",
       "  201.0,\n",
       "  966.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  444.0,\n",
       "  747.0,\n",
       "  0.0,\n",
       "  0.1573190581037597,\n",
       "  0.8511666666666671,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8511666666666671,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22],\n",
       " [8,\n",
       "  1,\n",
       "  1,\n",
       "  20.1,\n",
       "  19.4,\n",
       "  22.8,\n",
       "  19.4,\n",
       "  79.0,\n",
       "  79.0,\n",
       "  71.0,\n",
       "  87.0,\n",
       "  0.0,\n",
       "  201.0,\n",
       "  966.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  444.0,\n",
       "  747.0,\n",
       "  0.0,\n",
       "  0.1573190581037597,\n",
       "  1.3706666666666665,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3706666666666665,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22],\n",
       " [8,\n",
       "  1,\n",
       "  1,\n",
       "  20.1,\n",
       "  19.4,\n",
       "  22.8,\n",
       "  19.4,\n",
       "  79.0,\n",
       "  79.0,\n",
       "  71.0,\n",
       "  87.0,\n",
       "  0.0,\n",
       "  201.0,\n",
       "  966.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  444.0,\n",
       "  747.0,\n",
       "  0.0,\n",
       "  0.1573190581037597,\n",
       "  1.0185241699218762e-07,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0185241699218762e-07,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22],\n",
       " [8,\n",
       "  1,\n",
       "  1,\n",
       "  20.1,\n",
       "  19.4,\n",
       "  22.8,\n",
       "  19.4,\n",
       "  79.0,\n",
       "  79.0,\n",
       "  71.0,\n",
       "  87.0,\n",
       "  0.0,\n",
       "  201.0,\n",
       "  966.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  444.0,\n",
       "  747.0,\n",
       "  0.0,\n",
       "  0.1573190581037597,\n",
       "  1.9281666666666664,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.9281666666666664,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22],\n",
       " [8,\n",
       "  1,\n",
       "  1,\n",
       "  20.1,\n",
       "  19.4,\n",
       "  22.8,\n",
       "  19.4,\n",
       "  79.0,\n",
       "  79.0,\n",
       "  71.0,\n",
       "  87.0,\n",
       "  0.0,\n",
       "  201.0,\n",
       "  966.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  444.0,\n",
       "  747.0,\n",
       "  0.0,\n",
       "  0.1573190581037597,\n",
       "  0.5158833333333334,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5158833333333334,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22,\n",
       "  0.22]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n",
    "\n",
    "def evaluate():\n",
    "    print(\"Starting local evaluation\")\n",
    "    \n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    agent = OrderEnforcingAgent()\n",
    "\n",
    "    obs_dict = env_reset(env)\n",
    "\n",
    "    agent_time_elapsed = 0\n",
    "\n",
    "    step_start = time.perf_counter()\n",
    "    actions = agent.register_reset(obs_dict)\n",
    "    agent_time_elapsed += time.perf_counter()- step_start\n",
    "\n",
    "    episodes_completed = 0\n",
    "    num_steps = 0\n",
    "    interrupted = False\n",
    "    episode_metrics = []\n",
    "    try:\n",
    "        while True:\n",
    "            observations, _, done, _ = env.step(actions)\n",
    "            if done:\n",
    "                episodes_completed += 1\n",
    "                metrics_t = env.evaluate()\n",
    "                metrics = {\"price_cost\": metrics_t[0], \"emmision_cost\": metrics_t[1]}\n",
    "                if np.any(np.isnan(metrics_t)):\n",
    "                    raise ValueError(\"Episode metrics are nan, please contant organizers\")\n",
    "                episode_metrics.append(metrics)\n",
    "                print(f\"Episode complete: {episodes_completed} | Latest episode metrics: {metrics}\", )\n",
    "\n",
    "                obs_dict = env_reset(env)\n",
    "\n",
    "                step_start = time.perf_counter()\n",
    "                actions = agent.register_reset(obs_dict)\n",
    "                agent_time_elapsed += time.perf_counter()- step_start\n",
    "            else:\n",
    "                step_start = time.perf_counter()\n",
    "                actions = agent.compute_action(observations)\n",
    "                agent_time_elapsed += time.perf_counter()- step_start\n",
    "            \n",
    "            num_steps += 1\n",
    "            if num_steps % 1000 == 0:\n",
    "                print(f\"Num Steps: {num_steps}, Num episodes: {episodes_completed}\")\n",
    "\n",
    "            if episodes_completed >= Constants.episodes:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"========================= Stopping Evaluation =========================\")\n",
    "        interrupted = True\n",
    "    \n",
    "    if not interrupted:\n",
    "        print(\"=========================Completed=========================\")\n",
    "\n",
    "    if len(episode_metrics) > 0:\n",
    "        print(\"Average Price Cost:\", np.mean([e['price_cost'] for e in episode_metrics]))\n",
    "        print(\"Average Emmision Cost:\", np.mean([e['emmision_cost'] for e in episode_metrics]))\n",
    "    print(f\"Total time taken by agent: {agent_time_elapsed}s\")\n",
    "    \n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     evaluate()\n",
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citylearn_2022",
   "language": "python",
   "name": "citylearn_2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

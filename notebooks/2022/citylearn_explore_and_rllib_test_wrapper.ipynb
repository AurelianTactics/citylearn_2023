{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e369cfa-079b-4d61-a9a0-7fb67a90df63",
   "metadata": {},
   "source": [
    "### Simple script with two goals\n",
    "1.) Basic exploration of City Learn env\n",
    "\n",
    "2.) Create wrapper so taht City Learn env works with rllib: https://docs.ray.io/en/latest/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae77ee4-7f18-45bd-aa0a-8a209d36e0c6",
   "metadata": {},
   "source": [
    "#### Basic exploration of City Learn env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1e62ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting local evaluation\n",
      "=========================Completed=========================\n",
      "Total time taken by agent: 0.00045527098700404167s\n"
     ]
    }
   ],
   "source": [
    "# copied from old version of local_evaluate.py\n",
    "    # https://gitlab.aicrowd.com/aicrowd/challenges/citylearn-challenge-2022/citylearn-2022-starter-kit/-/blob/master/local_evaluation.py\n",
    "# just seeing the env basics\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Please do not make changes to this file. \n",
    "This is only a reference script provided to allow you \n",
    "to do local evaluation. The evaluator **DOES NOT** \n",
    "use this script for orchestrating the evaluations. \n",
    "\"\"\"\n",
    "\n",
    "from agents.orderenforcingwrapper import OrderEnforcingAgent\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    building_info = env.get_building_information()\n",
    "    building_info = list(building_info.values())\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "                \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n",
    "\n",
    "# def evaluate():\n",
    "print(\"Starting local evaluation\")\n",
    "\n",
    "env = CityLearnEnv(schema=Constants.schema_path)\n",
    "agent = OrderEnforcingAgent()\n",
    "\n",
    "obs_dict = env_reset(env)\n",
    "\n",
    "agent_time_elapsed = 0\n",
    "\n",
    "step_start = time.perf_counter()\n",
    "actions = agent.register_reset(obs_dict)\n",
    "agent_time_elapsed += time.perf_counter()- step_start\n",
    "\n",
    "episodes_completed = 0\n",
    "num_steps = 0\n",
    "interrupted = False\n",
    "episode_metrics = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        ### This is only a reference script provided to allow you \n",
    "        ### to do local evaluation. The evaluator **DOES NOT** \n",
    "        ### use this script for orchestrating the evaluations. \n",
    "\n",
    "        #observations, _, done, _ = env.step(actions)\n",
    "        # edit: seeing what reward and info does, and adding a break\n",
    "        observations, reward, done, info = env.step(actions)\n",
    "        break\n",
    "        if done:\n",
    "            episodes_completed += 1\n",
    "            metrics_t = env.evaluate()\n",
    "            metrics = {\"price_cost\": metrics_t[0], \"emmision_cost\": metrics_t[1]}\n",
    "            if np.any(np.isnan(metrics_t)):\n",
    "                raise ValueError(\"Episode metrics are nan, please contant organizers\")\n",
    "            episode_metrics.append(metrics)\n",
    "            print(f\"Episode complete: {episodes_completed} | Latest episode metrics: {metrics}\", )\n",
    "\n",
    "            obs_dict = env_reset(env)\n",
    "\n",
    "            step_start = time.perf_counter()\n",
    "            actions = agent.register_reset(obs_dict)\n",
    "            agent_time_elapsed += time.perf_counter()- step_start\n",
    "        else:\n",
    "            step_start = time.perf_counter()\n",
    "            actions = agent.compute_action(observations)\n",
    "            agent_time_elapsed += time.perf_counter()- step_start\n",
    "\n",
    "        num_steps += 1\n",
    "        if num_steps % 1000 == 0:\n",
    "            print(f\"Num Steps: {num_steps}, Num episodes: {episodes_completed}\")\n",
    "\n",
    "        if episodes_completed >= Constants.episodes:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"========================= Stopping Evaluation =========================\")\n",
    "    interrupted = True\n",
    "\n",
    "if not interrupted:\n",
    "    print(\"=========================Completed=========================\")\n",
    "\n",
    "if len(episode_metrics) > 0:\n",
    "    print(\"Average Price Cost:\", np.mean([e['price_cost'] for e in episode_metrics]))\n",
    "    print(\"Average Emmision Cost:\", np.mean([e['emmision_cost'] for e in episode_metrics]))\n",
    "print(f\"Total time taken by agent: {agent_time_elapsed}s\")\n",
    "\n",
    "\n",
    "# evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2698139c-bb0b-442a-98cc-8c6374aae2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----reset is \n",
      "{'action_space': [{'high': array([1.], dtype=float32), 'low': array([-1.], dtype=float32), 'shape': (1,), 'dtype': 'float32'}, {'high': array([1.], dtype=float32), 'low': array([-1.], dtype=float32), 'shape': (1,), 'dtype': 'float32'}, {'high': array([1.], dtype=float32), 'low': array([-1.], dtype=float32), 'shape': (1,), 'dtype': 'float32'}, {'high': array([1.], dtype=float32), 'low': array([-1.], dtype=float32), 'shape': (1,), 'dtype': 'float32'}, {'high': array([1.], dtype=float32), 'low': array([-1.], dtype=float32), 'shape': (1,), 'dtype': 'float32'}], 'observation_space': [{'high': array([  13.       ,    8.       ,   25.       ,   33.2      ,\n",
      "         33.2      ,   33.2      ,   33.2      ,  101.       ,\n",
      "        101.       ,  101.       ,  101.       , 1018.       ,\n",
      "       1018.       , 1018.       , 1018.       ,  954.       ,\n",
      "        954.       ,  954.       ,  954.       ,    1.2817962,\n",
      "          8.987483 ,  977.25     ,    2.       ,  966.23175  ,\n",
      "          1.54     ,    1.54     ,    1.54     ,    1.54     ],\n",
      "      dtype=float32), 'low': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  4.5999999e+00,\n",
      "        4.5999999e+00,  4.5999999e+00,  4.5999999e+00,  9.0000000e+00,\n",
      "        9.0000000e+00,  9.0000000e+00,  9.0000000e+00, -1.0000000e+00,\n",
      "       -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "       -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -9.2961711e-01,\n",
      "       -9.4300002e-01, -1.0000000e+00, -1.0000000e+00, -9.6623175e+02,\n",
      "       -7.9000002e-01, -7.9000002e-01, -7.9000002e-01, -7.9000002e-01],\n",
      "      dtype=float32), 'shape': (28,), 'dtype': 'float32'}, {'high': array([  13.       ,    8.       ,   25.       ,   33.2      ,\n",
      "         33.2      ,   33.2      ,   33.2      ,  101.       ,\n",
      "        101.       ,  101.       ,  101.       , 1018.       ,\n",
      "       1018.       , 1018.       , 1018.       ,  954.       ,\n",
      "        954.       ,  954.       ,  954.       ,    1.2817962,\n",
      "          7.8431334,  787.0833   ,    2.       ,  778.6006   ,\n",
      "          1.54     ,    1.54     ,    1.54     ,    1.54     ],\n",
      "      dtype=float32), 'low': array([   0.       ,    0.       ,    0.       ,    4.6      ,\n",
      "          4.6      ,    4.6      ,    4.6      ,    9.       ,\n",
      "          9.       ,    9.       ,    9.       ,   -1.       ,\n",
      "         -1.       ,   -1.       ,   -1.       ,   -1.       ,\n",
      "         -1.       ,   -1.       ,   -1.       ,   -0.9296171,\n",
      "         -0.9999999,   -1.       ,   -1.       , -778.6006   ,\n",
      "         -0.79     ,   -0.79     ,   -0.79     ,   -0.79     ],\n",
      "      dtype=float32), 'shape': (28,), 'dtype': 'float32'}, {'high': array([  13.       ,    8.       ,   25.       ,   33.2      ,\n",
      "         33.2      ,   33.2      ,   33.2      ,  101.       ,\n",
      "        101.       ,  101.       ,  101.       , 1018.       ,\n",
      "       1018.       , 1018.       , 1018.       ,  954.       ,\n",
      "        954.       ,  954.       ,  954.       ,    1.2817962,\n",
      "          7.101333 ,  844.7125   ,    2.       ,  836.2957   ,\n",
      "          1.54     ,    1.54     ,    1.54     ,    1.54     ],\n",
      "      dtype=float32), 'low': array([ 0.000000e+00,  0.000000e+00,  0.000000e+00,  4.600000e+00,\n",
      "        4.600000e+00,  4.600000e+00,  4.600000e+00,  9.000000e+00,\n",
      "        9.000000e+00,  9.000000e+00,  9.000000e+00, -1.000000e+00,\n",
      "       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,\n",
      "       -1.000000e+00, -1.000000e+00, -1.000000e+00, -9.296171e-01,\n",
      "       -9.999999e-01, -1.000000e+00, -1.000000e+00, -8.362957e+02,\n",
      "       -7.900000e-01, -7.900000e-01, -7.900000e-01, -7.900000e-01],\n",
      "      dtype=float32), 'shape': (28,), 'dtype': 'float32'}, {'high': array([  13.       ,    8.       ,   25.       ,   33.2      ,\n",
      "         33.2      ,   33.2      ,   33.2      ,  101.       ,\n",
      "        101.       ,  101.       ,  101.       , 1018.       ,\n",
      "       1018.       , 1018.       , 1018.       ,  954.       ,\n",
      "        954.       ,  954.       ,  954.       ,    1.2817962,\n",
      "          7.7496166,  707.0367   ,    2.       ,  698.29034  ,\n",
      "          1.54     ,    1.54     ,    1.54     ,    1.54     ],\n",
      "      dtype=float32), 'low': array([   0.       ,    0.       ,    0.       ,    4.6      ,\n",
      "          4.6      ,    4.6      ,    4.6      ,    9.       ,\n",
      "          9.       ,    9.       ,    9.       ,   -1.       ,\n",
      "         -1.       ,   -1.       ,   -1.       ,   -1.       ,\n",
      "         -1.       ,   -1.       ,   -1.       ,   -0.9296171,\n",
      "         -0.9999999,   -1.       ,   -1.       , -698.29034  ,\n",
      "         -0.79     ,   -0.79     ,   -0.79     ,   -0.79     ],\n",
      "      dtype=float32), 'shape': (28,), 'dtype': 'float32'}, {'high': array([  13.       ,    8.       ,   25.       ,   33.2      ,\n",
      "         33.2      ,   33.2      ,   33.2      ,  101.       ,\n",
      "        101.       ,  101.       ,  101.       , 1018.       ,\n",
      "       1018.       , 1018.       , 1018.       ,  954.       ,\n",
      "        954.       ,  954.       ,  954.       ,    1.2817962,\n",
      "          5.9387665,  819.2875   ,    2.       ,  808.85864  ,\n",
      "          1.54     ,    1.54     ,    1.54     ,    1.54     ],\n",
      "      dtype=float32), 'low': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  4.5999999e+00,\n",
      "        4.5999999e+00,  4.5999999e+00,  4.5999999e+00,  9.0000000e+00,\n",
      "        9.0000000e+00,  9.0000000e+00,  9.0000000e+00, -1.0000000e+00,\n",
      "       -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
      "       -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -9.2961711e-01,\n",
      "       -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -8.0885864e+02,\n",
      "       -7.9000002e-01, -7.9000002e-01, -7.9000002e-01, -7.9000002e-01],\n",
      "      dtype=float32), 'shape': (28,), 'dtype': 'float32'}], 'building_info': [{'solar_power': 4.0, 'annual_dhw_demand': 0.0, 'annual_cooling_demand': 0.0, 'annual_heating_demand': 0.0, 'annual_nonshiftable_electrical_demand': 0.001, 'correlations_dhw': {'86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_cooling_demand': {'86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_heating_demand': {'86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_non_shiftable_load': {'86ef2cf97d90495b909aa4656ad415cf': 0.196, '533345b395a34639acf75ec1571e5f72': 0.192, 'd0fb94d5244e40f09091c9c9fd9eecf4': 0.166, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': 0.25}}, {'solar_power': 4.0, 'annual_dhw_demand': 0.0, 'annual_cooling_demand': 0.0, 'annual_heating_demand': 0.0, 'annual_nonshiftable_electrical_demand': 0.001, 'correlations_dhw': {'165781c365784c99b86a51ae83a0f809': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_cooling_demand': {'165781c365784c99b86a51ae83a0f809': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_heating_demand': {'165781c365784c99b86a51ae83a0f809': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_non_shiftable_load': {'165781c365784c99b86a51ae83a0f809': 0.196, '533345b395a34639acf75ec1571e5f72': 0.25, 'd0fb94d5244e40f09091c9c9fd9eecf4': 0.196, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': 0.274}}, {'solar_power': 4.0, 'annual_dhw_demand': 0.0, 'annual_cooling_demand': 0.0, 'annual_heating_demand': 0.0, 'annual_nonshiftable_electrical_demand': 0.001, 'correlations_dhw': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_cooling_demand': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_heating_demand': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_non_shiftable_load': {'165781c365784c99b86a51ae83a0f809': 0.192, '86ef2cf97d90495b909aa4656ad415cf': 0.25, 'd0fb94d5244e40f09091c9c9fd9eecf4': 0.258, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': 0.262}}, {'solar_power': 5.0, 'annual_dhw_demand': 0.0, 'annual_cooling_demand': 0.0, 'annual_heating_demand': 0.0, 'annual_nonshiftable_electrical_demand': 0.001, 'correlations_dhw': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_cooling_demand': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_heating_demand': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': nan}, 'correlations_non_shiftable_load': {'165781c365784c99b86a51ae83a0f809': 0.166, '86ef2cf97d90495b909aa4656ad415cf': 0.196, '533345b395a34639acf75ec1571e5f72': 0.258, 'e0f2a2ddbadd438ab5c2e8b1cbeaae2b': 0.485}}, {'solar_power': 4.0, 'annual_dhw_demand': 0.0, 'annual_cooling_demand': 0.0, 'annual_heating_demand': 0.0, 'annual_nonshiftable_electrical_demand': 0.001, 'correlations_dhw': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan}, 'correlations_cooling_demand': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan}, 'correlations_heating_demand': {'165781c365784c99b86a51ae83a0f809': nan, '86ef2cf97d90495b909aa4656ad415cf': nan, '533345b395a34639acf75ec1571e5f72': nan, 'd0fb94d5244e40f09091c9c9fd9eecf4': nan}, 'correlations_non_shiftable_load': {'165781c365784c99b86a51ae83a0f809': 0.25, '86ef2cf97d90495b909aa4656ad415cf': 0.274, '533345b395a34639acf75ec1571e5f72': 0.262, 'd0fb94d5244e40f09091c9c9fd9eecf4': 0.485}}], 'observation': [[7, 7, 24, 20.0, 18.3, 22.8, 20.0, 84.0, 81.0, 68.0, 81.0, 0.0, 25.0, 964.0, 0.0, 0.0, 100.0, 815.0, 0.0, 0.1707244126403808, 2.2758, 0.0, 0.0, 2.2758, 0.22, 0.22, 0.22, 0.22], [7, 7, 24, 20.0, 18.3, 22.8, 20.0, 84.0, 81.0, 68.0, 81.0, 0.0, 25.0, 964.0, 0.0, 0.0, 100.0, 815.0, 0.0, 0.1707244126403808, 2.18875, 0.0, 0.0, 2.18875, 0.22, 0.22, 0.22, 0.22], [7, 7, 24, 20.0, 18.3, 22.8, 20.0, 84.0, 81.0, 68.0, 81.0, 0.0, 25.0, 964.0, 0.0, 0.0, 100.0, 815.0, 0.0, 0.1707244126403808, 1.0096232096354177e-07, 0.0, 0.0, 1.0096232096354177e-07, 0.22, 0.22, 0.22, 0.22], [7, 7, 24, 20.0, 18.3, 22.8, 20.0, 84.0, 81.0, 68.0, 81.0, 0.0, 25.0, 964.0, 0.0, 0.0, 100.0, 815.0, 0.0, 0.1707244126403808, 2.81915, 0.0, 0.0, 2.81915, 0.22, 0.22, 0.22, 0.22], [7, 7, 24, 20.0, 18.3, 22.8, 20.0, 84.0, 81.0, 68.0, 81.0, 0.0, 25.0, 964.0, 0.0, 0.0, 100.0, 815.0, 0.0, 0.1707244126403808, 0.7714333333333336, 0.0, 0.0, 0.7714333333333336, 0.22, 0.22, 0.22, 0.22]]}\n",
      "-----actions look like  [array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32)]\n",
      "-----reward look like  [-0.5513907  -0.74740795 -0.23022933 -0.95776332 -0.4248819 ]\n",
      "-----done look like  False\n",
      "-----info look like  {}\n",
      "observations look like  [[8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 0.8511666666666671, 0.0, 0.08685821630977214, 1.4613380482517289, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 1.3706666666666665, 0.0, 0.08685821630977214, 1.9808380482517283, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 1.0185241699218762e-07, 0.0, 0.08685821630977214, 0.6101714834374788, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 1.9281666666666664, 0.0, 0.08685821630977214, 2.5383380482517284, 0.22, 0.22, 0.22, 0.22], [8, 1, 1, 20.1, 19.4, 22.8, 19.4, 79.0, 79.0, 71.0, 87.0, 0.0, 201.0, 966.0, 0.0, 0.0, 444.0, 747.0, 0.0, 0.1573190581037597, 0.5158833333333334, 0.0, 0.08685821630977214, 1.1260547149183953, 0.22, 0.22, 0.22, 0.22]]\n"
     ]
    }
   ],
   "source": [
    "# env basics\n",
    "#observations, reward, done, info = env.step(actions)\n",
    "print(\"-----reset is \")\n",
    "print(obs_dict)\n",
    "print(\"-----actions look like \", actions)\n",
    "print(\"-----reward look like \", reward)\n",
    "print(\"-----done look like \", done)\n",
    "print(\"-----info look like \", info)\n",
    "print(\"observations look like \", observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afcd18a-0954-4860-b8d7-cb2161e4ef7a",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Reward is a list, one reward for each building. Typically reward is a single scalar.\n",
    "* Done and info are as expected\n",
    "* actions is a list of arrays, one for each building. If doing a single agent approach need to combine into one expected action\n",
    "* Unclear how to best use the information returned by the obs_dict on a reset call\n",
    "* Observation is a list of observations with a similar observation for each building\n",
    "    * To do: scale the obs, understand the obs, find out if obs space shifts for buildings, see what is actually different in the obs for each building, see which parts of the obs space is shared between buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f232c4-c598-44db-8b6d-1f90ca9e3232",
   "metadata": {},
   "source": [
    "#### Create rllib wrapper for City Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f2405e-a470-4456-92a3-e2c43c65714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Confirm that rllib installed correctly\n",
    "# ## this works but commenting out because output is huge\n",
    "import ray\n",
    "from ray import tune\n",
    "# from ray.rllib.algorithms import ppo\n",
    "\n",
    "# tune.run(\n",
    "#     \"PPO\",\n",
    "#     stop={\"timesteps_total\": 1000},\n",
    "#     config={\n",
    "#         \"env\": \"CartPole-v1\",\n",
    "#         \"framework\": \"torch\",\n",
    "#         \"num_gpus\": 1,\n",
    "#         \"num_workers\": 0,\n",
    "#         \"lr\": 0.001,\n",
    "#         #\"output\": \"/tmp/out\", \n",
    "#         \"batch_mode\": \"complete_episodes\"\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca6555c-77d8-46fc-b0c9-8bbf3021981e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "gym.__version__\n",
    "\n",
    "# city learn installs gym version 24 but ray requires version < 22. Need to see if compatibility is an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af88e875-e277-48ca-946b-8daf85632822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 04:07:12,711\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    }
   ],
   "source": [
    "# Create a wrapper for City Learn taht works with rllib and test the wrapper \n",
    "\n",
    "import gym\n",
    "import ray\n",
    "import numpy as np\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "\n",
    "class SingleAgentCityLearnEnv(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # env.observation_space is a list of Box spaces, one for each building\n",
    "        min_space = np.zeros((len(env.observation_space), env.observation_space[0].shape[0]), dtype='float32')\n",
    "        max_space = np.zeros((len(env.observation_space), env.observation_space[0].shape[0]), dtype='float32')\n",
    "        for i in range(len(env.observation_space)):\n",
    "            min_space[i] = env.observation_space[i].low\n",
    "            max_space[i] = env.observation_space[i].high\n",
    "        self.observation_space = gym.spaces.Box(low=min_space, high=max_space, shape=(len(env.observation_space), env.observation_space[0].shape[0] ),\n",
    "                                                                                     dtype=np.float32)\n",
    "        \n",
    "        # to do: other phases unclear what the sizes of the actions pace is going to be\n",
    "        # get action space by iterating over given env. Each space is a list of boxes\n",
    "        min_space = np.ones((len(env.action_space),), dtype='float32') * -1\n",
    "        max_space = np.ones((len(env.action_space),), dtype='float32')\n",
    "        for i in range(len(env.action_space)):\n",
    "            min_space[i] = env.action_space[i].low[0]\n",
    "            max_space[i] = env.action_space[i].high[0]\n",
    "        # https://github.com/openai/gym/blob/master/gym/spaces/box.py\n",
    "        self.action_space = gym.spaces.Box(low=min_space, high=max_space, shape=(len(env.action_space),), dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        # is a list of where each obs in obs_list is the observation of a building\n",
    "        obs_list = self.env.reset() \n",
    "        obs = self._shape_obs(obs_list)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # CL env expects action as a list of actions where each action is of shape (1,)\n",
    "        #sample_action = [array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32)]\n",
    "        #a = np.array([0.01])\n",
    "        #sample_action = [a]*5\n",
    "        action_list = self._shape_action(action)\n",
    "        obs_list, reward_list, done, info = self.env.step(action_list)\n",
    "        \n",
    "        # reward is a list for each building. summing over all rewards but this loses some useful info\n",
    "        reward = self._shape_reward(reward_list)\n",
    "        \n",
    "        obs = self._shape_obs(obs_list)\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def _shape_obs(self, obs_list):\n",
    "        '''\n",
    "        Turns obs from list of building obs into a combined observations\n",
    "        '''\n",
    "        # to do scale the obs passed on self.observation_space\n",
    "        obs = np.array(obs_list, dtype='float32')\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _shape_action(self, action):\n",
    "        '''\n",
    "        Turns action from Box of full action space into what CL env expects action to be\n",
    "        CL env expects action as a list of actions where each action is of shape (1,)\n",
    "        sample_action = [array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32), array([0.091], dtype=float32)]\n",
    "        a = np.array([0.01])\n",
    "        sample_action = [a]*5\n",
    "        '''\n",
    "        action_list = []\n",
    "        for action_index in range(self.action_space.shape[0]):\n",
    "            action_list.append([action[action_index]])\n",
    "        # print(action_list)\n",
    "        \n",
    "        return action_list\n",
    "    \n",
    "    def _shape_reward(self, reward_list):\n",
    "        reward = np.sum(reward_list)\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    \n",
    "cl_env = CityLearnEnv(schema='citylearn_challenge_2022_phase_1')\n",
    "sa_env = SingleAgentCityLearnEnv(cl_env)\n",
    "ray.rllib.utils.check_env(sa_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129238b-f166-4ef0-b313-0adccb9e53c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=9364)\u001b[0m 2022-07-31 04:07:49,820\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=9364)\u001b[0m 2022-07-31 04:07:49,820\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=9364)\u001b[0m 2022-07-31 04:07:50,178\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:07:50 (running for 00:00:03.27)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=9364)\u001b[0m 2022-07-31 04:07:50,247\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:07:55 (running for 00:00:08.28)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:08:00 (running for 00:00:13.28)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:08:05 (running for 00:00:18.28)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:08:10 (running for 00:00:23.29)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:08:15 (running for 00:00:28.29)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:08:20 (running for 00:00:33.29)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 04:08:24,277\tWARNING tune.py:682 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:08:25 (running for 00:00:38.30)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-31 04:08:25 (running for 00:00:38.30)<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/4 CPUs, 0/1 GPUs, 0.0/4.5 GiB heap, 0.0/2.25 GiB objects (0.0/1.0 accelerator_type:GTX)<br>Result logdir: /home/jim/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_citylearn_dcdee_00000</td><td>RUNNING </td><td>10.0.0.63:9364</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "\n",
    "def env_creator_sa(args):\n",
    "    schema = 'citylearn_challenge_2022_phase_1'\n",
    "    env = CityLearnEnv(schema=schema)\n",
    "    sa_env = SingleAgentCityLearnEnv(env)\n",
    "    \n",
    "    return sa_env\n",
    "\n",
    "env = env_creator_sa({})\n",
    "register_env(\"citylearn\", env_creator_sa)\n",
    "\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"timesteps_total\": 1000},\n",
    "    config={\n",
    "        \"env\": \"citylearn\",#\"CartPole-v1\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"num_gpus\": 0, #debug why this does not work when set to 1\n",
    "        \"num_workers\": 0,\n",
    "        \"lr\": 0.001,\n",
    "        #\"output\": \"/tmp/out\", \n",
    "        \"batch_mode\": \"complete_episodes\"\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citylearn_2022",
   "language": "python",
   "name": "citylearn_2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
